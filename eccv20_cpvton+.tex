% updated April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016; AAS, 2020

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}

% INITIAL SUBMISSION - The following two lines are NOT commented
% CAMERA READY - Comment OUT the following two lines
\usepackage{ruler}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}


\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter

\def\ECCVSubNumber{\dots}  % Insert your submission number here

\title{Image-based Virtual Try-On: its limitations and an improvement} % Replace with your title

% INITIAL SUBMISSION 
%\begin{comment}
\titlerunning{ECCV-20 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-20 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
%\end{comment}
%******************

% CAMERA READY SUBMISSION
\begin{comment}
\titlerunning{3D Reconstruction of Clothes ... VTON}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Matiur Rahman Minar\inst{1}\orcidID{0000-0002-3128-2915} \and
Thai Thanh Tuan\inst{2,3}\orcidID{1111-2222-3333-4444} \and
Heejune Ahn\inst{3}\orcidID{2222--3333-4444-5555}  \and
Paul Rosin\inst{3}\orcidID{2222--3333-4444-5555}   \and
Yukun Lai\inst{3}\orcidID{2222--3333-4444-5555}  }
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Seoul National University of Science and Technology, Seoul  08544, South Korea \and
Cardiff University, Cardiff, 69121 Heidelberg, UK
\email{heejune@seoultech.ac.kr}\\
\url{http://www.springer.com/gp/computer-science/lncs}}
\end{comment}
%******************
\maketitle

\begin{abstract}

 Recently, image-based virtual try-on (VTON) technology has
drawn increasing research attraction for practical online apparel shopping.
Many previous studies assume the input of a try-on cloth and a
target human image and 2-step pipeline: first, it warps the try-on cloth to
align with the target human, and then blends the warped cloth with the
target human image. The contributions of this work are three-fold. First,
despite the successful demonstration of the previous work, the input data
condition for successful performance has not yet well investigated. Thus, we conduct a performance study with classified inputs in cloth style, and human pose and shape. reveals that the current image-based VTON algorithms have a limited working condition. Second, through our detailed examination of the classified experiment, we identify the problem in the dataset such as improper human segmentation labeling, improper matching with user demands, such as not retaining non-targeted cloth, and weakness of the state-of-the-art algorithms in cloth warping and blending. Finally, we proposed a new image-based algorithm, named CP-VTON+, tackling the observed issues. The experiment proves shows CP-VTON+ consistent improvements in IoU and SSIM for the same cloth re-try-on, and IS and visual observation for new cloth try-on over the state-of-the-art algorithm.       

\keywords{Virtual Try-On, Image-based, Deep-Learning, Quality Comparison}
\end{abstract}

\include{sec1}  % intro
\include{sec2}  % Classified analysis
\include{sec3}  % CPVTON+ 
\include{sec4}  % Experiments


\section{Conclusions}

Recently, image-based virtual try-on (VTON) technology has
drawn increasing research attraction for practical online apparel shopping.
Many previous studies assume a pair of a try-on cloth and a
target human image as input data and constructed as a 2-step pipeline: first, it warps the try-on cloth to
align with the target human and then blends the warped cloth with the
target human image. The contributions of this work are three-fold. First,
despite the successful demonstration of the previous work, the input data
condition for successful performance has not yet well investigated. Thus, we conduct a performance study with classified inputs in cloth style, and human pose and shape. reveals that the current image-based VTON algorithms have a limited working condition. Second, through our detailed examination of the classified experiment, we identify the problem in the dataset such as improper human segmentation labeling, improper matching with user demands, such as not retaining non-targeted cloth, and weakness of the state-of-the-art algorithms in cloth warping and blending. Finally, we proposed an improvement from the state-of-the-art algorithm, CP-VTON,  thus named CP-VTON+, tackling the observed issues. The experiment proves shows CP-VTON+ consistent improvements in IoU and SSIM for the same cloth re-try-on, and IS and visual observation for new cloth try-on over the state-of-the-art algorithm.
  
     
\clearpage
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{cpvton+bib}



\end{document}
